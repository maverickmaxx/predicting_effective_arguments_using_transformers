‌@article{FerrettiRalphP2019Awta,
abstract = {Despite the early emergence of oral argumentation, written argumentation is slow to develop, insensitive to alternative perspectives, and generally of poor quality. These findings are unsettling because high quality argumentative writing is expected throughout the curriculum and needed in an increasingly competitive workplace that requires advanced communication skills. In this introduction, we provide background about the theoretical perspectives that inform the papers included in this special issue and highlight their contributions to the extant literature about argumentative writing.},
author = {Ferretti, Ralph P and Graham, Steve},
address = {Dordrecht},
copyright = {Springer Nature B.V. 2019},
issn = {0922-4777},
journal = {Reading & writing},
keywords = {Argumentation ; Article ; Communication ; Curricula ; Education ; Educational theory ; general ; Language and Literature ; Linguistics ; Literacy ; Neurology ; Psycholinguistics ; Social Sciences ; Writing instruction ; Written language},
language = {eng},
number = {6},
pages = {1345-1357},
publisher = {Springer Netherlands},
title = {Argumentative writing: theory, assessment, and instruction},
volume = {32},
year = {2019}
}

@report{NCESWriting2011, 
title={The nation’s report card: Writing 2011 (NCES 2012-470)},
journal={Ed.gov}, publisher={National Center for Education Statistics, U.S: Department of Education}, year={2012},
address={Washington, D.C.}
}

@report{espinoza_saunders_kini_darling-hammond_2018, 
title={Taking the Long View: State Efforts to Solve Teacher Shortages by Strengthening the Profession}, 
url={https://edworkingpapers.org/sites/default/files/State%20Efforts%20to%20Solve%20Teacher%20Shortages.pdf}, 
author={Espinoza, Daniel and Saunders, Ryan and Kini, Tara and Darling-Hammond, Linda}, 
address={Palo Alto, CA},
publisher={Learning Policy Institute},
year={2018} 
}

‌‌@misc{KaggleFBRubric2022, title={argumentation_scheme_and_rubrics_kaggle.docx}, url={https://docs.google.com/document/d/1G51Ulb0i-nKCRQSs4p4ujauy4wjAJOae/edit}, journal={Google Docs}, author={argumentation_scheme_and_rubrics_kaggle.docx}, year={2022} }


@book{alma991033112569304706,
address = {New York},
booktitle = {Best practices in writing instruction},
edition = {Third edition.},
isbn = {9781462538003},
keywords = {English language -- Composition and exercises -- Study and teaching (Elementary)},
language = {eng},
publisher = {The Guilford Press},
title = {Best practices in writing instruction },
year = {2019 - 2019},
}

@misc{KaggleFBPrize2022, title={feedback prize - predicting effective arguments | kaggle_2022}, url={https://www.kaggle.com/competitions/feedback-prize-effectiveness}, journal={Kaggle.com}, year={2022} }

@article{doi:10.1177/0741088398015002004,
author = {JOANNA G. CRAMMOND},
title ={The Uses and Complexity of Argument Structures in Expert and Student Persuasive Writing},
journal = {Written Communication},
volume = {15},
number = {2},
pages = {230-268},
year = {1998},
doi = {10.1177/0741088398015002004},

URL = { 
        https://doi.org/10.1177/0741088398015002004
    
},
eprint = { 
        https://doi.org/10.1177/0741088398015002004
    
}
,
    abstract = { This study investigated differences among student writers at three grade levels (6, 8, and 10) and between expert writers and students in terms of the uses and complexity of arguments presented in their persuasive texts. To analyze argument, a model was developed that could account for structural variations occurring across a range of writing situations. The characteristics of this model were defined using categories derived from a model of semantic representation in discourse. The structural analysis revealed that (a) argument was the predominant organizational structure for all writers, (b) more than 80\% of students produced arguments involving some form of opposition, (c) embedded arguments identified in expert texts functioned primarily as countered rebuttals and in student texts as subclaims or reservations, and (d) expert texts contained relatively higher frequencies of warrants, countered rebuttals, and modals, and student uses of these substructures increased with grade. }
}

@misc{ChildsBhatnagarAli_2022, title={Help! Thought Process Request}, url={https://docs.google.com/document/d/1CYCn2gfS_2tJ4s9I4MFyLosaS2XgoC9px1tilAc9V-s/edit?usp=sharing}, author={Childs, Ali, Waqas}, year={2022} }

@article{RankinJoanL.1993Iwab,
abstract = {This study examined relations between self‐efficacy and outcome expectancy beliefs and spelling and writing performance. Perceptions about spelling and writing were assessed in 258 collegeage participants. Spelling performance was measured through a 50‐item spelling test and writing performance by a holistically scored writing sample. The most highly correlated variables included spelling outcome expectancy and writing outcome expectancy, spelling selfefficacy and writing self‐efficacy, spelling performance and spelling self‐efficacy, and spelling and writing performance. A causal model relating perceptions, spelling performance, and writing performance was proposed and its appropriateness estimated. Direct effects on spelling were found for spelling self‐efficacy, while spelling self‐efficacy had indirect effects on writing performance and spelling had a direct effect on writing performance. The causal model was discussed in terms of changing conceptions of writing instruction and traditional views of the role of spelling as a necessary component of good writing.},
author = {Rankin, Joan L. and Bruning, Roger H. and Timme, Vicky L. and Katkanant, Chanida},
address = {West Sussex},
copyright = {Copyright © 1993 John Wiley & Sons, Ltd},
issn = {0888-4080},
journal = {Applied cognitive psychology},
keywords = {Beliefs ; Biological and medical sciences ; Factors ; Fundamental and applied biological sciences. Psychology ; Language ; Performance ; Production and perception of written language ; Psychology. Psychoanalysis. Psychiatry ; Psychology. Psychophysiology ; Spelling ; Writing},
language = {eng},
number = {2},
pages = {155-169},
publisher = {John Wiley & Sons, Ltd},
title = {Is writing affected by spelling performance and beliefs about spelling?},
volume = {7},
year = {1993},
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@techreport{Santorini1990,
  added-at = {2014-03-26T23:25:56.000+0100},
  author = {Santorini, Beatrice},
  biburl = {https://www.bibsonomy.org/bibtex/234cdf6ddadd89376090e7dada2fc18ec/butonic},
  file = {:Santorini - Penn Treebank tag definitions.pdf:PDF},
  institution = {Department of Computer and Information Science, University of Pennsylvania},
  interhash = {818e72efd9e4b5fae3e51e88848100a0},
  intrahash = {34cdf6ddadd89376090e7dada2fc18ec},
  keywords = {dis pos tagging treebank},
  number = {MS-CIS-90-47},
  timestamp = {2014-03-26T23:25:56.000+0100},
  title = {Part-of-speech tagging guidelines for the {P}enn {T}reebank {P}roject},
  url = {ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz},
  year = 1990
}

@misc{HePengchengDeBerta2020,
  doi = {10.48550/ARXIV.2006.03654},
  
  url = {https://arxiv.org/abs/2006.03654},
  
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.7, cs.CL, cs.GL},
  
  title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{McNamaraDanielleS2015Ahca,
abstract = {•The authors evaluate the use of a hierarchical classification approach to AES.•Linguistic, semantic, and rhetorical features are computed using Coh-Metrix, LIWC, and WAT.•Overall, the models report exact and adjacent accuracy comparable to other studies.•Different features inform each level of the hierarchy, and consequently, each score.•A potential advantage of a hierarchical approach is to better inform feedback to the student.
This study evaluates the use of a hierarchical classification approach to automated assessment of essays. Automated essay scoring (AES) generally relies on machine learning techniques that compute essay scores using a set of text variables. Unlike previous studies that rely on regression models, this study computes essay scores using a hierarchical approach, analogous to an incremental algorithm for hierarchical classification. The corpus in this study consists of 1243 argumentative (persuasive) essays written on 14 different prompts, across 3 different grade levels (9th grade, 11th grade, college freshman), and four different time limits for writing or temporal conditions (untimed essays and essays written in 10, 15, and 25minute increments). The features included in the analysis are computed using the automated tools, Coh-Metrix, the Writing Assessment Tool (WAT), and Linguistic Inquiry and Word Count (LIWC). Overall, the models developed to score all the essays in the data set report 55% exact accuracy and 92% adjacent accuracy between the predicted essay scores and the human scores. The results indicate that this is a promising approach to AES that could provide more specific feedback to writers and may be relevant to other natural language computations, such as the scoring of short answers in comprehension or knowledge assessments.},
author = {McNamara, Danielle S and Crossley, Scott A and Roscoe, Rod D and Allen, Laura K and Dai, Jianmin},
copyright = {2014 Elsevier Ltd},
issn = {1075-2935},
journal = {Assessing writing},
keywords = {AES ; assessment ; Automated essay scoring ; computer programs ; Hierarchical classification ; rhetoric and composition ; student writing ; Writing assessment},
language = {eng},
number = {1},
pages = {35-59},
publisher = {Elsevier Inc},
title = {A hierarchical classification approach to automated essay scoring},
volume = {23},
year = {2015},
}

@incollection{MaJunteng2021EHSF,
abstract = {Automated Essay Scoring (AES) aims to evaluate the quality of an essay automatically. In practice, an essay is usually organized in a hierarchical structure, which means that the writer needs to organize the main ideas into different paragraphs, and organize coherent sentences and appropriate words for the essay. Therefore, it is crucial to model the hierarchical structure of essays for AES. For addressing this issue, most of the existing works used neural network-based architectures (e.g., CNNs and LSTMs) to model the hierarchical structure of essays. Different from previous studies, we propose a novel hierarchical graph structure based on graph convolutional networks (GCN) to encode the hierarchical structure of essays and hope to obtain those structured coherence and discourse information from the graph aggregation. We conduct several experiments on ASAP dataset and the experimental results demonstrate the effectiveness of our method.},
author = {Ma, Junteng and Li, Xia and Chen, Minping and Yang, Weigeng},
address = {Cham},
booktitle = {Information Retrieval},
copyright = {These electronic books are licensed by OhioLINK and may be under copyright protection. Please see the Acceptable Use Guidelines for more information, or contact your librarian.},
isbn = {9783030881887},
issn = {0302-9743},
keywords = {Automated Essay Scoring ; Graph neural network ; Hierarchical structure features},
language = {eng},
pages = {168-179},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {Enhanced Hierarchical Structure Features for Automated Essay Scoring},
year = {2021},
}

@article{LinTsung-Yi2020FLfD,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
address = {United States},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Computer vision ; Convolutional neural networks ; Detectors ; Entropy ; Feature extraction ; machine learning ; Object detection ; Proposals ; Training},
language = {eng},
number = {2},
pages = {318-327},
publisher = {IEEE},
title = {Focal Loss for Dense Object Detection},
volume = {42},
year = {2020},
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{anantgupt_2022, title={[PyTorch] FeedBack (eda+train) DeBerta V3}, url={https://www.kaggle.com/code/anantgupt/pytorch-feedback-eda-train-deberta-v3?scriptVersionId=102108552}, journal={Kaggle.com}, publisher={Kaggle}, author={anantgupt}, year={2022}, month={Jul} }

@article{WeidingerLaura2021Easr,
abstract = {This paper aims to help structure the risk landscape associated with
large-scale Language Models (LMs). In order to foster advances in responsible
innovation, an in-depth understanding of the potential risks posed by these
models is needed. A wide range of established and anticipated risks are
analysed in detail, drawing on multidisciplinary expertise and literature from
computer science, linguistics, and social sciences.
We outline six specific risk areas: I. Discrimination, Exclusion and
Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious
Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and
Environmental Harms. The first area concerns the perpetuation of stereotypes,
unfair discrimination, exclusionary norms, toxic language, and lower
performance by social group for LMs. The second focuses on risks from private
data leaks or LMs correctly inferring sensitive information. The third
addresses risks arising from poor, false or misleading information including in
sensitive domains, and knock-on risks such as the erosion of trust in shared
information. The fourth considers risks from actors who try to use LMs to cause
harm. The fifth focuses on risks specific to LLMs used to underpin
conversational agents that interact with human users, including unsafe use,
manipulation or deception. The sixth discusses the risk of environmental harm,
job automation, and other challenges that may have a disparate effect on
different social groups or communities.
In total, we review 21 risks in-depth. We discuss the points of origin of
different risks and point to potential mitigation approaches. Lastly, we
discuss organisational responsibilities in implementing mitigations, and the
role of collaboration and participation. We highlight directions for further
research, particularly on expanding the toolkit for assessing and evaluating
the outlined risks in LMs.},
author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
copyright = {http://creativecommons.org/licenses/by/4.0},
language = {eng},
title = {Ethical and social risks of harm from Language Models},
year = {2021},
}

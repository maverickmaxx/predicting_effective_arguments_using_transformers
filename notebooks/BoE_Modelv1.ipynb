{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c9d3f0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d316a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\waqasali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "from spellchecker import SpellChecker\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from regex import E\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253836c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_berkeley.csv') \n",
    "test_df = pd.read_csv('../data/test_berkeley.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbeead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33292</th>\n",
       "      <td>36760</td>\n",
       "      <td>9f63b687e76a</td>\n",
       "      <td>FFA381E58FC6</td>\n",
       "      <td>For many people they don't like only asking on...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33293</th>\n",
       "      <td>36761</td>\n",
       "      <td>9d5bd7d86212</td>\n",
       "      <td>FFA381E58FC6</td>\n",
       "      <td>also people have different views and opinions ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33294</th>\n",
       "      <td>36762</td>\n",
       "      <td>f1b78becd573</td>\n",
       "      <td>FFA381E58FC6</td>\n",
       "      <td>Advice is something that can impact a persons ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33295</th>\n",
       "      <td>36763</td>\n",
       "      <td>cc184624ca8e</td>\n",
       "      <td>FFA381E58FC6</td>\n",
       "      <td>someone can use everything that many people sa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33296</th>\n",
       "      <td>36764</td>\n",
       "      <td>c8a973681feb</td>\n",
       "      <td>FFA381E58FC6</td>\n",
       "      <td>In conclusion asking for an opinion can be ben...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33297 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  discourse_id      essay_id  \\\n",
       "0               0  0013cc385424  007ACE74B050   \n",
       "1               1  9704a709b505  007ACE74B050   \n",
       "2               2  c22adee811b6  007ACE74B050   \n",
       "3               3  a10d361e54e4  007ACE74B050   \n",
       "4               4  db3e453ec4e2  007ACE74B050   \n",
       "...           ...           ...           ...   \n",
       "33292       36760  9f63b687e76a  FFA381E58FC6   \n",
       "33293       36761  9d5bd7d86212  FFA381E58FC6   \n",
       "33294       36762  f1b78becd573  FFA381E58FC6   \n",
       "33295       36763  cc184624ca8e  FFA381E58FC6   \n",
       "33296       36764  c8a973681feb  FFA381E58FC6   \n",
       "\n",
       "                                          discourse_text  \\\n",
       "0      Hi, i'm Isaac, i'm going to be writing about h...   \n",
       "1      On my perspective, I think that the face is a ...   \n",
       "2      I think that the face is a natural landform be...   \n",
       "3      If life was on Mars, we would know by now. The...   \n",
       "4      People thought that the face was formed by ali...   \n",
       "...                                                  ...   \n",
       "33292  For many people they don't like only asking on...   \n",
       "33293  also people have different views and opinions ...   \n",
       "33294  Advice is something that can impact a persons ...   \n",
       "33295  someone can use everything that many people sa...   \n",
       "33296  In conclusion asking for an opinion can be ben...   \n",
       "\n",
       "             discourse_type discourse_effectiveness  \n",
       "0                      Lead                Adequate  \n",
       "1                  Position                Adequate  \n",
       "2                     Claim                Adequate  \n",
       "3                  Evidence                Adequate  \n",
       "4              Counterclaim                Adequate  \n",
       "...                     ...                     ...  \n",
       "33292                 Claim                Adequate  \n",
       "33293                 Claim                Adequate  \n",
       "33294              Position                Adequate  \n",
       "33295              Evidence             Ineffective  \n",
       "33296  Concluding Statement             Ineffective  \n",
       "\n",
       "[33297 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa29296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>218bfd96197d</td>\n",
       "      <td>0168325D0E24</td>\n",
       "      <td>Being in a car is how we transport ourselves t...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>599e5e16e9ed</td>\n",
       "      <td>0168325D0E24</td>\n",
       "      <td>Well Paris has enforced a driving ban to help ...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>a398be97f2b7</td>\n",
       "      <td>0168325D0E24</td>\n",
       "      <td>It actually helped having cars stop driving on...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>38a6679de2ed</td>\n",
       "      <td>0168325D0E24</td>\n",
       "      <td>\"Recent studies suggest that Americans are buy...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>f1cc93a81a32</td>\n",
       "      <td>0168325D0E24</td>\n",
       "      <td>In the more populated citys, people are using ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>36725</td>\n",
       "      <td>8d66664512a0</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>they might want to give the on who is giving t...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>36726</td>\n",
       "      <td>4c649f487587</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>they are give you as much detail as they have ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>36727</td>\n",
       "      <td>b887de306ae7</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>They will want to give you all the information...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>36728</td>\n",
       "      <td>54851ec259ee</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>If a person wants good advice they will want t...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>36729</td>\n",
       "      <td>b7c27670f3bc</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>why you need advice you will talk more then th...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3468 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  discourse_id      essay_id  \\\n",
       "0             69  218bfd96197d  0168325D0E24   \n",
       "1             70  599e5e16e9ed  0168325D0E24   \n",
       "2             71  a398be97f2b7  0168325D0E24   \n",
       "3             72  38a6679de2ed  0168325D0E24   \n",
       "4             73  f1cc93a81a32  0168325D0E24   \n",
       "...          ...           ...           ...   \n",
       "3463       36725  8d66664512a0  FE3CA06DDCA1   \n",
       "3464       36726  4c649f487587  FE3CA06DDCA1   \n",
       "3465       36727  b887de306ae7  FE3CA06DDCA1   \n",
       "3466       36728  54851ec259ee  FE3CA06DDCA1   \n",
       "3467       36729  b7c27670f3bc  FE3CA06DDCA1   \n",
       "\n",
       "                                         discourse_text        discourse_type  \\\n",
       "0     Being in a car is how we transport ourselves t...                  Lead   \n",
       "1     Well Paris has enforced a driving ban to help ...              Evidence   \n",
       "2     It actually helped having cars stop driving on...                 Claim   \n",
       "3     \"Recent studies suggest that Americans are buy...              Evidence   \n",
       "4     In the more populated citys, people are using ...                 Claim   \n",
       "...                                                 ...                   ...   \n",
       "3463  they might want to give the on who is giving t...                 Claim   \n",
       "3464  they are give you as much detail as they have ...                 Claim   \n",
       "3465  They will want to give you all the information...              Evidence   \n",
       "3466  If a person wants good advice they will want t...              Evidence   \n",
       "3467  why you need advice you will talk more then th...  Concluding Statement   \n",
       "\n",
       "     discourse_effectiveness  \n",
       "0                   Adequate  \n",
       "1                   Adequate  \n",
       "2                   Adequate  \n",
       "3                   Adequate  \n",
       "4                  Effective  \n",
       "...                      ...  \n",
       "3463                Adequate  \n",
       "3464                Adequate  \n",
       "3465                Adequate  \n",
       "3466                Adequate  \n",
       "3467                Adequate  \n",
       "\n",
       "[3468 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e40fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adequate', 'Ineffective', 'Effective'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = train_df.discourse_effectiveness.unique()\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(datasets):\n",
    "    for dataset in datasets:\n",
    "        for _, text in dataset:\n",
    "            yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(build_vocab([train_dataset, test_dataset]), specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8be7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fcb1119",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IterableWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\waqasali\\OneDrive - Intel Corporation\\Documents\\MIDS\\W207\\w207_final_project\\notebooks\\FFNN-BoE_Modelv1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/waqasali/OneDrive%20-%20Intel%20Corporation/Documents/MIDS/W207/w207_final_project/notebooks/FFNN-BoE_Modelv1.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/waqasali/OneDrive%20-%20Intel%20Corporation/Documents/MIDS/W207/w207_final_project/notebooks/FFNN-BoE_Modelv1.ipynb#ch0000022?line=2'>3</a>\u001b[0m train_dataset, test_dataset  \u001b[39m=\u001b[39m torchtext\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mAG_NEWS()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/waqasali/OneDrive%20-%20Intel%20Corporation/Documents/MIDS/W207/w207_final_project/notebooks/FFNN-BoE_Modelv1.ipynb#ch0000022?line=4'>5</a>\u001b[0m target_classes \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWorld\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSports\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBusiness\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSci/Tec\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\waqasali\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[1;34m(root, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(new_root):\n\u001b[0;32m    192\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(new_root, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m fn(root\u001b[39m=\u001b[39mnew_root, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\waqasali\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[1;34m(root, split, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m _check_default_set(split, splits, fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[1;32m--> 155\u001b[0m     result\u001b[39m.\u001b[39mappend(fn(root, item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m _wrap_datasets(\u001b[39mtuple\u001b[39m(result), split)\n",
      "File \u001b[1;32mc:\\Users\\waqasali\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\datasets\\ag_news.py:71\u001b[0m, in \u001b[0;36mAG_NEWS\u001b[1;34m(root, split)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchdata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     67\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 71\u001b[0m url_dp \u001b[39m=\u001b[39m IterableWrapper([URL[split]])\n\u001b[0;32m     72\u001b[0m cache_dp \u001b[39m=\u001b[39m url_dp\u001b[39m.\u001b[39mon_disk_cache(\n\u001b[0;32m     73\u001b[0m     filepath_fn\u001b[39m=\u001b[39mpartial(_filepath_fn, root, split),\n\u001b[0;32m     74\u001b[0m     hash_dict\u001b[39m=\u001b[39m{_filepath_fn(root, split): MD5[split]},\n\u001b[0;32m     75\u001b[0m     hash_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m cache_dp \u001b[39m=\u001b[39m HttpReader(cache_dp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IterableWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset, test_dataset  = torchtext.datasets.AG_NEWS()\n",
    "\n",
    "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tec\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a702d678",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\waqasali\\OneDrive - Intel Corporation\\Documents\\MIDS\\W207\\w207_final_project\\notebooks\\FFNN-BoE_Modelv1.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/waqasali/OneDrive%20-%20Intel%20Corporation/Documents/MIDS/W207/w207_final_project/notebooks/FFNN-BoE_Modelv1.ipynb#ch0000023?line=0'>1</a>\u001b[0m TEXT \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mField(tokenize\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mspacy\u001b[39m\u001b[39m'\u001b[39m,batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,include_lengths\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/waqasali/OneDrive%20-%20Intel%20Corporation/Documents/MIDS/W207/w207_final_project/notebooks/FFNN-BoE_Modelv1.ipynb#ch0000023?line=1'>2</a>\u001b[0m LABEL \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mLabelField(dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat,batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01648062",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c23e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 40 # number of features\n",
    "hidden_size = 200 \n",
    "num_classes = 3\n",
    "num_epochs = 10\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc16fe4",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd95948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  7., 12.,  2.,  2.,  1.,  2., 12.,  2.,  8.,  5.,  2.,  3.,  1.,\n",
      "         1.,  4.,  2.,  3.,  2.,  2., 76.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]) tensor([1., 0., 0.])\n",
      "tensor([[ 1.,  0.,  4.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  5.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  4.,  5.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 2.,  0.,  5.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  3.,  ...,  0.,  0.,  0.],\n",
      "        [ 2.,  2., 11.,  ...,  0.,  0.,  0.]]) tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        # read data with numpy\n",
    "        xy = np.loadtxt(path, delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        # The first 3 columns are the class label, the rest are the features\n",
    "        self.x_data = torch.from_numpy(xy[:, 3:]) # size [n_samples, n_features]\n",
    "        self.y_data = torch.from_numpy(xy[:, :3]) # size [n_samples, 3]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# create dataset\n",
    "train_dataset = FeaturesDataset('../data/processed/train_features.csv')\n",
    "test_dataset = FeaturesDataset('../data/processed/test_features.csv')\n",
    "\n",
    "# get first sample and unpack\n",
    "first_data = train_dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n",
    "\n",
    "# Load whole dataset with DataLoader\n",
    "# shuffle: shuffle data, good for training\n",
    "# num_workers: faster loading with multiple subprocesses\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "\n",
    "# convert to an iterator and look at one random sample\n",
    "train_iter = iter(train_loader)\n",
    "data = train_iter.next()\n",
    "features, labels = data\n",
    "print(features, labels)\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "#data = test_iter.next()\n",
    "#features, labels = data\n",
    "#print(features, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c0bdd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc531c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size         \n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7cfeba",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a23f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # it automatically applies the softmax to the output of last layer. \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5d63a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db199804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SAMPLES =  33297 NUMBER OF ITERATIONS =  67\n",
      "Epoch [1/10], Step [50/67], Loss: 0.9669\n",
      "Epoch [2/10], Step [50/67], Loss: 0.9610\n",
      "Epoch [3/10], Step [50/67], Loss: 0.8298\n",
      "Epoch [4/10], Step [50/67], Loss: 0.8942\n",
      "Epoch [5/10], Step [50/67], Loss: 0.8705\n",
      "Epoch [6/10], Step [50/67], Loss: 0.8905\n",
      "Epoch [7/10], Step [50/67], Loss: 0.8698\n",
      "Epoch [8/10], Step [50/67], Loss: 0.8543\n",
      "Epoch [9/10], Step [50/67], Loss: 0.8772\n",
      "Epoch [10/10], Step [50/67], Loss: 0.8235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(\"NUMBER OF SAMPLES = \",str(total_samples) + \" NUMBER OF ITERATIONS = \",n_iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # here: 33297 samples, batch_size = 500, n_iters=33297/500=66.5 -> 67 iterations\n",
    "        # Run the training process\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_iterations}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f991d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b036b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy : 63.350634371395614 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        # max returns (value, index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Model Accuracy : {acc} %')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "537e6b82cffd710c047f8e87e1af201701deab2d61e049ada42b0315ac2ac2d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

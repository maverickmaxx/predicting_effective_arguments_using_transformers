{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c6dfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\vibhatna\\appdata\\local\\continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages (0.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b5a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29cbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v6.5'\n",
    "\n",
    "MAX_LEN_DISCOURSE_TEXT = 256\n",
    "MAX_LEN_ESSAY = 512\n",
    "TRAIN_BATCH_SIZE  = 18\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "DROP_OUT = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "LEARNING_RATE = 6e-6\n",
    "\n",
    "\n",
    "BERT_LAYERS = 3\n",
    "BERT_PATH = './bert_base_cased'\n",
    "MODEL_PATH = './Model/model' + VERSION + '.bin'\n",
    "\n",
    "TRAINING_FILE =  '../Data/train_berkeley.csv'\n",
    "TEST_FILE = '../Data/test_berkeley.csv'\n",
    "ESSAY_FOLDER = '../feedback-prize-effectiveness/train'\n",
    "\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    BERT_PATH,\n",
    "    do_lower_case=False\n",
    ")\n",
    "\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    'Adequate': 1,\n",
    "    'Effective': 0,\n",
    "    'Ineffective' : 2\n",
    "}\n",
    "\n",
    "DISCOURSE_TYPE_MAPPING = {\n",
    "    'Lead': 0,\n",
    "    'Position': 1,\n",
    "    'Claim' : 2,\n",
    "    'Evidence' : 3,\n",
    "    'Counterclaim' : 4,\n",
    "    'Rebuttal' : 5,\n",
    "    'Concluding Statement' : 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b843c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 1000 rows\n",
      "Processed 2000 rows\n",
      "Processed 3000 rows\n",
      "Processed 4000 rows\n",
      "Processed 5000 rows\n",
      "Processed 6000 rows\n",
      "Processed 7000 rows\n",
      "Processed 8000 rows\n",
      "Processed 9000 rows\n",
      "Processed 10000 rows\n",
      "Processed 11000 rows\n",
      "Processed 12000 rows\n",
      "Processed 13000 rows\n",
      "Processed 14000 rows\n",
      "Processed 15000 rows\n",
      "Processed 16000 rows\n",
      "Processed 17000 rows\n",
      "Processed 18000 rows\n",
      "Processed 19000 rows\n",
      "Processed 20000 rows\n",
      "Processed 21000 rows\n",
      "Processed 22000 rows\n",
      "Processed 23000 rows\n",
      "Processed 24000 rows\n",
      "Processed 25000 rows\n",
      "Processed 26000 rows\n",
      "Processed 27000 rows\n",
      "Processed 28000 rows\n",
      "Processed 29000 rows\n",
      "Processed 30000 rows\n",
      "Processed 31000 rows\n",
      "Processed 32000 rows\n",
      "Processed 33000 rows\n",
      "{10: 11.0, 25: 18.0, 50: 32.0, 90: 119.0, 95: 160.0, 99: 264.0, 99.9: 468.7040000000052, 99.99: 727.8271999998033}\n",
      "{10: 259.0, 25: 346.0, 50: 486.0, 90: 950.2000000000044, 95: 1088.0, 99: 1260.0, 99.9: 1522.0, 99.99: 4779.0}\n",
      "{10: 293.0, 25: 386.0, 50: 535.0, 90: 1025.0, 95: 1167.0, 99: 1405.0800000000017, 99.9: 1698.0, 99.99: 4803.703999999954}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train = pd.read_csv(TRAINING_FILE)\n",
    "discourse_text_len = []\n",
    "essay_len = []\n",
    "combined_len = []\n",
    "\n",
    "index = 0\n",
    "for text in df_train.discourse_text.values:\n",
    "    \n",
    "    if index % 1000 == 0:\n",
    "        print(f'Processed {index} rows')\n",
    "    \n",
    "    text_tokenized_len = len(TOKENIZER.tokenize(text))    \n",
    "    discourse_text_len.append(text_tokenized_len)\n",
    "    \n",
    "    essay_id = df_train.essay_id.values[index]\n",
    "    essay_path = os.path.join(ESSAY_FOLDER, f\"{essay_id}.txt\")\n",
    "    essay = open(essay_path, 'r').read()\n",
    "    essay_tokenized_len = len(TOKENIZER.tokenize(essay))\n",
    "    essay_len.append(essay_tokenized_len)\n",
    "    \n",
    "    combined_len.append(text_tokenized_len + essay_tokenized_len)\n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "def get_stats(lengths, percentile):\n",
    "    stats = {}\n",
    "    for p in percentile:\n",
    "        stats[p] = np.percentile(np.array(lengths), p)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "percentile = [10, 25, 50, 90, 95, 99, 99.9, 99.99]\n",
    "\n",
    "print(get_stats(discourse_text_len, percentile))\n",
    "print(get_stats(essay_len, percentile))\n",
    "print(get_stats(combined_len, percentile))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dc98eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "\n",
    "ps = PorterStemmer()\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a4331f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"okay\" in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e96d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'many', 'sides', 'to', 'people', 'giving', 'up', 'their', 'cars', '.', 'Some', 'people', 'are', 'truly', 'happy', 'and', 'some', 'are', 'not', '.', 'It', 'may', 'not', 'be', 'that', 'bad', ',', 'i', 'mean', 'how', 'did', 'people', 'manige', 'before', 'cars', 'were', 'even', 'invented']\n",
      "['there', 'are', 'many', 'sides', 'to', 'people', 'giving', 'up', 'their', 'cars', 'some', 'people', 'are', 'truly', 'happy', 'and', 'some', 'are', 'not', 'it', 'may', 'not', 'be', 'that', 'bad', 'i', 'mean', 'how', 'did', 'people', 'manige', 'before', 'cars', 'were', 'even', 'invented']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'There are many sides to people giving up their cars. Some people are truly happy and some are not. It may not be that bad, i mean how did people manige before cars were even invented'\n",
    "\n",
    "sentence_tokenize_before = word_tokenize(sentence)\n",
    "print(sentence_tokenize_before)\n",
    "\n",
    "sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "sentence =sentence.lower()\n",
    "\n",
    "sentence_tokenize_after = word_tokenize(sentence)\n",
    "print(sentence_tokenize_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8142954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "Processed 200000 words\n",
      "Processed 300000 words\n",
      "Processed 400000 words\n",
      "Processed 500000 words\n",
      "Processed 600000 words\n",
      "Processed 700000 words\n",
      "Processed 800000 words\n",
      "Processed 900000 words\n",
      "Processed 1000000 words\n",
      "Processed 1100000 words\n",
      "49815\n",
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "Processed 200000 words\n",
      "261552\n",
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "346423\n"
     ]
    }
   ],
   "source": [
    "word_set = {'sample'}\n",
    "index = 0\n",
    "for word in brown.words():\n",
    "    \n",
    "    word = word.lower()\n",
    "    \n",
    "    if index % 100000 == 0:\n",
    "        print(f'Processed {index} words')\n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "    if not word in word_set:\n",
    "        word_set.add(word)\n",
    "\n",
    "print(len(word_set))\n",
    "\n",
    "\n",
    "index = 0\n",
    "for word in words.words():\n",
    "    \n",
    "    word = word.lower()\n",
    "    \n",
    "    if index % 100000 == 0:\n",
    "        print(f'Processed {index} words')\n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "    if not word in word_set:\n",
    "        word_set.add(word)\n",
    "        \n",
    "print(len(word_set))\n",
    "\n",
    "\n",
    "index = 0\n",
    "for word in wordnet.words():\n",
    "    \n",
    "    word = word.lower()\n",
    "    \n",
    "    if index % 100000 == 0:\n",
    "        print(f'Processed {index} words')\n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "    if not word in word_set:\n",
    "        word_set.add(word)\n",
    "        \n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd9ecf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 300 rows\n",
      "Processed 600 rows\n",
      "Processed 900 rows\n",
      "Processed 1200 rows\n",
      "Processed 1500 rows\n",
      "Processed 1800 rows\n",
      "Processed 2100 rows\n",
      "Processed 2400 rows\n",
      "Processed 2700 rows\n",
      "Processed 3000 rows\n",
      "Processed 3300 rows\n",
      "Processed 3600 rows\n",
      "Processed 3900 rows\n",
      "Processed 4200 rows\n",
      "Processed 4500 rows\n",
      "Processed 4800 rows\n",
      "Processed 5100 rows\n",
      "Processed 5400 rows\n",
      "Processed 5700 rows\n",
      "Processed 6000 rows\n",
      "Processed 6300 rows\n",
      "Processed 6600 rows\n",
      "Processed 6900 rows\n",
      "Processed 7200 rows\n",
      "Processed 7500 rows\n",
      "Processed 7800 rows\n",
      "Processed 8100 rows\n",
      "Processed 8400 rows\n",
      "Processed 8700 rows\n",
      "Processed 9000 rows\n",
      "Processed 9300 rows\n",
      "Processed 9600 rows\n",
      "Processed 9900 rows\n",
      "Processed 10200 rows\n",
      "Processed 10500 rows\n",
      "Processed 10800 rows\n",
      "Processed 11100 rows\n",
      "Processed 11400 rows\n",
      "Processed 11700 rows\n",
      "Processed 12000 rows\n",
      "Processed 12300 rows\n",
      "Processed 12600 rows\n",
      "Processed 12900 rows\n",
      "Processed 13200 rows\n",
      "Processed 13500 rows\n",
      "Processed 13800 rows\n",
      "Processed 14100 rows\n",
      "Processed 14400 rows\n",
      "Processed 14700 rows\n",
      "Processed 15000 rows\n",
      "Processed 15300 rows\n",
      "Processed 15600 rows\n",
      "Processed 15900 rows\n",
      "Processed 16200 rows\n",
      "Processed 16500 rows\n",
      "Processed 16800 rows\n",
      "Processed 17100 rows\n",
      "Processed 17400 rows\n",
      "Processed 17700 rows\n",
      "Processed 18000 rows\n",
      "Processed 18300 rows\n",
      "Processed 18600 rows\n",
      "Processed 18900 rows\n",
      "Processed 19200 rows\n",
      "Processed 19500 rows\n",
      "Processed 19800 rows\n",
      "Processed 20100 rows\n",
      "Processed 20400 rows\n",
      "Processed 20700 rows\n",
      "Processed 21000 rows\n",
      "Processed 21300 rows\n",
      "Processed 21600 rows\n",
      "Processed 21900 rows\n",
      "Processed 22200 rows\n",
      "Processed 22500 rows\n",
      "Processed 22800 rows\n",
      "Processed 23100 rows\n",
      "Processed 23400 rows\n",
      "Processed 23700 rows\n",
      "Processed 24000 rows\n",
      "Processed 24300 rows\n",
      "Processed 24600 rows\n",
      "Processed 24900 rows\n",
      "Processed 25200 rows\n",
      "Processed 25500 rows\n",
      "Processed 25800 rows\n",
      "Processed 26100 rows\n",
      "Processed 26400 rows\n",
      "Processed 26700 rows\n",
      "Processed 27000 rows\n",
      "Processed 27300 rows\n",
      "Processed 27600 rows\n",
      "Processed 27900 rows\n",
      "Processed 28200 rows\n",
      "Processed 28500 rows\n",
      "Processed 28800 rows\n",
      "Processed 29100 rows\n",
      "Processed 29400 rows\n",
      "Processed 29700 rows\n",
      "Processed 30000 rows\n",
      "Processed 30300 rows\n",
      "Processed 30600 rows\n",
      "Processed 30900 rows\n",
      "Processed 31200 rows\n",
      "Processed 31500 rows\n",
      "Processed 31800 rows\n",
      "Processed 32100 rows\n",
      "Processed 32400 rows\n",
      "Processed 32700 rows\n",
      "Processed 33000 rows\n"
     ]
    }
   ],
   "source": [
    "def count_nii_words(text):\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    count = 0    \n",
    "    words = []\n",
    "    \n",
    "    for token in tokens:        \n",
    "        if not token in word_set and not str.isdigit(token):\n",
    "            token_stem = ps.stem(token)\n",
    "            if not token_stem in word_set:\n",
    "                count += 1\n",
    "                words.append(token)\n",
    "\n",
    "    return count, words\n",
    "    \n",
    "\n",
    "def count_spelling_isses(text):\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    misspelled = spell.unknown(tokens)\n",
    "    return len(misspelled)\n",
    "\n",
    "    \n",
    "df_valid = pd.read_csv(TRAINING_FILE)\n",
    "df_valid['not_in_dictionary'] = -1\n",
    "df_valid['not_in_dictionary_words'] = ''\n",
    "df_valid['not_in_dictionary_essay'] = -1\n",
    "df_valid['not_in_dictionary_words_essay'] = ''\n",
    "df_valid['spelling_issues'] = -1\n",
    "\n",
    "index = 0\n",
    "for text in df_valid.discourse_text.values:\n",
    "    \n",
    "    if index % 300 == 0:\n",
    "        print(f'Processed {index} rows')\n",
    "    \n",
    "    count, words = count_nii_words(text)\n",
    "    \n",
    "    df_valid['not_in_dictionary'][index] = count\n",
    "    df_valid['not_in_dictionary_words'][index] = ';'.join(words)\n",
    "    \n",
    "    \n",
    "    essay_id = df_valid.essay_id.values[index]\n",
    "    essay_path = os.path.join(ESSAY_FOLDER, f\"{essay_id}.txt\")\n",
    "    essay = open(essay_path, 'r').read()\n",
    "    \n",
    "    count, words = count_nii_words(essay)\n",
    "    \n",
    "    df_valid['not_in_dictionary_essay'][index] = count\n",
    "    df_valid['not_in_dictionary_words_essay'][index] = ';'.join(words)\n",
    "    \n",
    "    \n",
    "    count = -1\n",
    "    df_valid['spelling_issues'][index] = count\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "df_valid.to_csv('../Data/NII_spelling_essay_data_training.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdf7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270155b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8976d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7de9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789be8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea732b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020154c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6084c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52add5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs229project] *",
   "language": "python",
   "name": "conda-env-cs229project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

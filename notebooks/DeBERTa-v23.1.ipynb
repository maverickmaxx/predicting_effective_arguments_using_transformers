{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fc24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d327ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vibhatna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Import\n",
    "\n",
    "import tokenizers\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.models.deberta_v2 import DebertaV2Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sentencepiece\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import string\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c815441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "Processed 200000 words\n",
      "Processed 300000 words\n",
      "Processed 400000 words\n",
      "Processed 500000 words\n",
      "Processed 600000 words\n",
      "Processed 700000 words\n",
      "Processed 800000 words\n",
      "Processed 900000 words\n",
      "Processed 1000000 words\n",
      "Processed 1100000 words\n",
      "49815\n",
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "Processed 200000 words\n",
      "261552\n",
      "Processed 0 words\n",
      "Processed 100000 words\n",
      "346423\n"
     ]
    }
   ],
   "source": [
    "## Creating Word Set for Not In Index\n",
    "\n",
    "word_set = {'sample'}\n",
    "\n",
    "def add_words(word_set, words):\n",
    "    index = 0\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if index % 100000 == 0:\n",
    "            print(f'Processed {index} words')\n",
    "        index += 1    \n",
    "    \n",
    "        if not word in word_set:\n",
    "            word_set.add(word)\n",
    "        \n",
    "    print(len(word_set))\n",
    "    \n",
    "add_words(word_set, brown.words())\n",
    "add_words(word_set, words.words())\n",
    "add_words(word_set, wordnet.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd83851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "VERSION = 'v23.1'\n",
    "\n",
    "MAX_LEN_DISCOURSE_TEXT = 512\n",
    "TRAIN_BATCH_SIZE  = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 8\n",
    "DROP_OUT = 0.10\n",
    "TEST_SIZE = 0.1\n",
    "LEARNING_RATE = 3e-6\n",
    "\n",
    "\n",
    "BASE_MODEL = 'microsoft/deberta-v3-base'\n",
    "MODEL_PATH = './Model/model' + VERSION + '.bin'\n",
    "\n",
    "TRAINING_FILE =  '../Data/train_berkeley.csv'\n",
    "TEST_FILE = '../Data/test_berkeley.csv'\n",
    "ESSAY_FOLDER = '../feedback-prize-effectiveness/train'\n",
    "\n",
    "TOKENIZER = DebertaV2Tokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    'Adequate': 1,\n",
    "    'Effective': 0,\n",
    "    'Ineffective' : 2\n",
    "}\n",
    "\n",
    "DISCOURSE_TYPE_MAPPING = {\n",
    "    'Lead': 0,\n",
    "    'Position': 1,\n",
    "    'Claim' : 2,\n",
    "    'Evidence' : 3,\n",
    "    'Counterclaim' : 4,\n",
    "    'Rebuttal' : 5,\n",
    "    'Concluding Statement' : 6\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "print(f'Device: {device}')\n",
    "\n",
    "GPT_MODEL_ID = 'gpt2'  # can also change to gpt-2 large if size is not an issue\n",
    "GPT_MODEL = GPT2LMHeadModel.from_pretrained(GPT_MODEL_ID).to(device)\n",
    "GPT_TOKENIZER = GPT2TokenizerFast.from_pretrained(GPT_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba88b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class BERTDataset:\n",
    "    def __init__(self, discourse_texts, discourse_types, essay_ids, targets, essay_folder, max_len_discourse_text, word_set, discourse_types_text):\n",
    "        self.discourse_texts = discourse_texts\n",
    "        self.discourse_types = discourse_types\n",
    "        self.discourse_types_text = discourse_types_text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.essay_ids = essay_ids\n",
    "        self.max_len_discourse_text = max_len_discourse_text        \n",
    "        self.essay_folder = essay_folder\n",
    "        self.word_set = word_set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.discourse_texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        discourse_text = str(self.discourse_texts[item])\n",
    "        discourse_type = self.discourse_types[item]        \n",
    "        discourse_type_text = self.discourse_types_text[item]\n",
    "\n",
    "        # Counting Spelling errors in Text\n",
    "        text = discourse_text\n",
    "        text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "    \n",
    "        count = 0\n",
    "        for token in tokens:            \n",
    "            if not token in word_set and not str.isdigit(token):\n",
    "                token_stem = ps.stem(token)\n",
    "                if not token_stem in word_set:\n",
    "                    count += 1\n",
    "        \n",
    "        nii_count_discourse_text = (count - 0.85) / 1.70\n",
    "        \n",
    "        gpt_encodings = GPT_TOKENIZER(discourse_text, return_tensors='pt')\n",
    "        gpt_input_ids = gpt_encodings.input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            gpt_outputs = GPT_MODEL(gpt_input_ids, labels=gpt_input_ids)\n",
    "            perplexity =  float(torch.exp(gpt_outputs[0]))\n",
    "            perplexity = (perplexity - 230) / 1578\n",
    "        \n",
    "        \n",
    "        essay_id = self.essay_ids[item]\n",
    "        essay_path = os.path.join(self.essay_folder, f\"{essay_id}.txt\")\n",
    "        essay = open(essay_path, 'r').read()\n",
    "        \n",
    "        input_text = discourse_type_text + \" \" + self.tokenizer.sep_token + discourse_text + \" \" + self.tokenizer.sep_token + \" \" + essay\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            add_special_tokens=True,            \n",
    "            max_length=self.max_len_discourse_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask = True\n",
    "        )\n",
    "\n",
    "        ids = inputs.input_ids.flatten()\n",
    "        mask = inputs.attention_mask.flatten() \n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'discourse_type': torch.tensor(discourse_type, dtype=torch.long),\n",
    "            'target': torch.tensor(self.targets[item], dtype=torch.long),\n",
    "            'nii_count_discourse_text': torch.tensor(nii_count_discourse_text, dtype=torch.float),\n",
    "            'perplexity': torch.tensor(perplexity, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af1c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()  \n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)   \n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(BASE_MODEL, output_hidden_states=True)\n",
    "        self.model = AutoModel.from_pretrained(BASE_MODEL, config=self.config) \n",
    "        self.mpool = MeanPooling()\n",
    "        self.dropout = nn.Dropout(DROP_OUT)\n",
    "        self.hidden = nn.Linear(777, 256)\n",
    "        self.out = nn.Linear(256, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, ids, mask, discourse_type, nii_count_discourse_text, perplexity):\n",
    "        \n",
    "        discourse_text_output = self.model(input_ids=ids, attention_mask=mask)\n",
    "        discourse_text_hidden_states = self.mpool(discourse_text_output.last_hidden_state, mask)\n",
    "        model_output_discourse_text = self.dropout(discourse_text_hidden_states)\n",
    "        \n",
    "        discourse_type_output = F.one_hot(discourse_type, num_classes=len(DISCOURSE_TYPE_MAPPING))        \n",
    "        nii_count_discourse_text  = nii_count_discourse_text.view(-1,1)        \n",
    "        perplexity = perplexity.view(-1,1)\n",
    "        \n",
    "        merged_output = torch.cat((model_output_discourse_text,                                   \n",
    "                                   discourse_type_output,\n",
    "                                   nii_count_discourse_text,\n",
    "                                   perplexity), dim=1)      \n",
    "        \n",
    "        hidden = self.hidden(merged_output)\n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ab79bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in Train: 33297\n",
      "Total samples in Validation: 3468\n",
      "Total Batches in Train: 8325\n",
      "Total Batches in Valid: 1734\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE)\n",
    "df_train['label'] = df_train.discourse_effectiveness\n",
    "df_train.label = df_train.label.map(CLASS_MAPPING)\n",
    "df_train['discourse_type_int'] = df_train.discourse_type\n",
    "df_train.discourse_type_int = df_train.discourse_type_int.map(DISCOURSE_TYPE_MAPPING)\n",
    "print(f'Total samples in Train: {len(df_train)}')\n",
    "\n",
    "\n",
    "df_valid = pd.read_csv(TEST_FILE)\n",
    "df_valid['label'] = df_valid.discourse_effectiveness\n",
    "df_valid.label = df_valid.label.map(CLASS_MAPPING)\n",
    "df_valid['discourse_type_int'] = df_valid.discourse_type\n",
    "df_valid.discourse_type_int = df_valid.discourse_type_int.map(DISCOURSE_TYPE_MAPPING)\n",
    "print(f'Total samples in Validation: {len(df_valid)}')\n",
    "\n",
    "\n",
    "train_dataset = BERTDataset(\n",
    "    discourse_texts=df_train.discourse_text.values,\n",
    "    essay_ids = df_train.essay_id.values,\n",
    "    targets=df_train.label.values,\n",
    "    discourse_types = df_train.discourse_type_int.values,\n",
    "    essay_folder = ESSAY_FOLDER,\n",
    "    max_len_discourse_text = MAX_LEN_DISCOURSE_TEXT,\n",
    "    word_set = word_set,\n",
    "    discourse_types_text = df_train.discourse_type.values\n",
    ")\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = BERTDataset(\n",
    "    discourse_texts=df_valid.discourse_text.values,\n",
    "    essay_ids = df_valid.essay_id.values,\n",
    "    targets=df_valid.label.values,\n",
    "    discourse_types = df_valid.discourse_type_int.values,\n",
    "    essay_folder = ESSAY_FOLDER,\n",
    "    max_len_discourse_text = MAX_LEN_DISCOURSE_TEXT,\n",
    "    word_set = word_set,\n",
    "    discourse_types_text = df_valid.discourse_type.values\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Total Batches in Train: {len(train_data_loader)}')\n",
    "print(f'Total Batches in Valid: {len(valid_data_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea72bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_focal(output, target):\n",
    "    \n",
    "    gamma = torch.tensor(2.0).to(device, dtype=torch.float)\n",
    "    max_prob = torch.tensor(1.0).to(device, dtype=torch.float)\n",
    "    \n",
    "    output_softmax = torch.softmax(output, dim=1)\n",
    "    output_softmax_nlog = -1 * torch.log(output_softmax)\n",
    "    \n",
    "    target_onehot = F.one_hot(target, num_classes=3)\n",
    "    cross_entropy = torch.multiply(target_onehot, output_softmax_nlog)\n",
    "    \n",
    "    weight = torch.multiply(target_onehot, torch.pow(torch.subtract(max_prob, output_softmax), gamma))    \n",
    "    \n",
    "    focal_loss = torch.multiply(weight, cross_entropy)\n",
    "    loss = (focal_loss).sum(dim=1).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(output, target):\n",
    "    return nn.CrossEntropyLoss()(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9dc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhatna\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b9cfda1a8d4be482185e0fae438546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhatna\\AppData\\Local\\Temp\\ipykernel_17192\\3851347274.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(ids, dtype=torch.long),\n",
      "C:\\Users\\vibhatna\\AppData\\Local\\Temp\\ipykernel_17192\\3851347274.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask, dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752a9c6a2594310ba175dfb5e355146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 :: Training Loss: 0.7252, Validation Loss: 1.2346, Validation Accuracy: 0.6993\n",
      "Kaggle Loss: 0.6751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e90dc9b29cf41c98130e05e9a7f215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b344a0b0f48678a6b6a77cc41ef50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 :: Training Loss: 0.6026, Validation Loss: 1.2076, Validation Accuracy: 0.7050\n",
      "Kaggle Loss: 0.6668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e273b6cae34f0886c96a8d98e158ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbd659c492a412ba90951cd87691f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 :: Training Loss: 0.5128, Validation Loss: 1.1931, Validation Accuracy: 0.6990\n",
      "Kaggle Loss: 0.6688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a1737b06f4b3ab57839bcd1f4d903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2228f6c5f24f92adb1cef81c711bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 :: Training Loss: 0.4199, Validation Loss: 1.1838, Validation Accuracy: 0.6981\n",
      "Kaggle Loss: 0.7089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5bccc830324967ac9f84b4f8124c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m perplexity\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscourse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscourse_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnii_count_discourse_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnii_count_discourse_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperplexity\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[0;32m     70\u001b[0m loss_focal \u001b[38;5;241m=\u001b[39m loss_fn_focal(output, target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mBERTBaseUncased.forward\u001b[1;34m(self, ids, mask, discourse_type, nii_count_discourse_text, perplexity)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids, mask, discourse_type, nii_count_discourse_text, perplexity):\n\u001b[1;32m---> 27\u001b[0m     discourse_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     discourse_text_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmpool(discourse_text_output\u001b[38;5;241m.\u001b[39mlast_hidden_state, mask)\n\u001b[0;32m     29\u001b[0m     model_output_discourse_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(discourse_text_hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1049\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1039\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1041\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1042\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1043\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1046\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1047\u001b[0m )\n\u001b[1;32m-> 1049\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:498\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    489\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    490\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    491\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m         rel_embeddings,\n\u001b[0;32m    496\u001b[0m     )\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 498\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    508\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:342\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    335\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m ):\n\u001b[1;32m--> 342\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    351\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:273\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    266\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    272\u001b[0m ):\n\u001b[1;32m--> 273\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    282\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:698\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[0;32m    697\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[1;32m--> 698\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cs229project\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:742\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative position ids must be of dim 2 or 3 or 4. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_pos\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    741\u001b[0m att_span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_ebd_size\n\u001b[1;32m--> 742\u001b[0m relative_pos \u001b[38;5;241m=\u001b[39m \u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m rel_embeddings \u001b[38;5;241m=\u001b[39m rel_embeddings[\u001b[38;5;241m0\u001b[39m : att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_att_key:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BERTBaseUncased()\n",
    "model.to(device)\n",
    "#model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.008,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "optimizer = AdamW(optimizer_parameters, lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_accuracy = 0.0\n",
    "total_valid_loss = 0.0\n",
    "total_train_loss = 0.0\n",
    "print(f'Model Training Started')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        \n",
    "    total_train_loss = 0.0\n",
    "    # Train Function\n",
    "    \n",
    "    for batch_index, data in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "        ids = data['ids']\n",
    "        mask = data['mask']\n",
    "        target = data['target']        \n",
    "        discourse_type = data['discourse_type']\n",
    "        nii_count_discourse_text= data['nii_count_discourse_text']\n",
    "        perplexity = data['perplexity']\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "        discourse_type = discourse_type.to(device, dtype=torch.long)\n",
    "        nii_count_discourse_text = nii_count_discourse_text.to(device, dtype=torch.float)\n",
    "        perplexity = perplexity.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            discourse_type=discourse_type,\n",
    "            nii_count_discourse_text=nii_count_discourse_text,\n",
    "            perplexity=perplexity\n",
    "        )\n",
    "        \n",
    "        loss = loss_fn(output, target)\n",
    "        loss_focal = loss_fn_focal(output, target)\n",
    "        \n",
    "        total_loss = torch.add(loss, loss_focal)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    \n",
    "    total_valid_loss = 0.0    \n",
    "    total_correct = 0\n",
    "    kaggle_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_index, data in tqdm(enumerate(valid_data_loader), total=len(valid_data_loader)):\n",
    "            ids = data['ids']\n",
    "            mask = data['mask']\n",
    "            target = data['target']        \n",
    "            discourse_type = data['discourse_type']\n",
    "            nii_count_discourse_text= data['nii_count_discourse_text']\n",
    "            perplexity = data['perplexity']\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            target = target.to(device, dtype=torch.long)\n",
    "            discourse_type = discourse_type.to(device, dtype=torch.long)\n",
    "            nii_count_discourse_text = nii_count_discourse_text.to(device, dtype=torch.float)\n",
    "            perplexity = perplexity.to(device, dtype=torch.float)\n",
    "\n",
    "            output = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                discourse_type=discourse_type,\n",
    "                nii_count_discourse_text=nii_count_discourse_text,\n",
    "                perplexity=perplexity         \n",
    "            )\n",
    "\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            output_sum = torch.sum(output, dim=1)\n",
    "            for batch_index in range(len(target)):\n",
    "                kaggle_loss += -1 * torch.log(output[batch_index][target[batch_index]] / output_sum[batch_index])\n",
    "\n",
    "\n",
    "            validloss = loss_fn(output, target)            \n",
    "            validloss_focal = loss_fn_focal(output, target)        \n",
    "            total_loss = torch.add(validloss, validloss_focal)            \n",
    "            total_valid_loss += total_loss.item()\n",
    "            \n",
    "            labels_prediction_conf, labels_prediction_class = torch.max(output, 1)\n",
    "            total_correct += torch.sum(labels_prediction_class == target).item()\n",
    "            validation_accuracy = total_correct / len(valid_dataset)\n",
    "        \n",
    "        \n",
    "        total_valid_loss = total_valid_loss / len(valid_data_loader)\n",
    "        total_train_loss = total_train_loss / len(train_data_loader)        \n",
    "        print(f'Epoch: {epoch + 1} :: Training Loss: {total_train_loss:.4f}, Validation Loss: {total_valid_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}')\n",
    "        print(f'Kaggle Loss: {kaggle_loss / len(valid_dataset):.4f}')        \n",
    "        \n",
    "        \n",
    "        if validation_accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_accuracy = validation_accuracy\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a21969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0976d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs229project] *",
   "language": "python",
   "name": "conda-env-cs229project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

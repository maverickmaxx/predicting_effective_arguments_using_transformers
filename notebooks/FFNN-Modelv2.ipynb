{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c9d3f0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d316a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/ds_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/adam/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "from spellchecker import SpellChecker\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from regex import E\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01648062",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c23e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 49 # number of features\n",
    "hidden_size = 200 \n",
    "num_classes = 3\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "learning_rate = 0.00001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc16fe4",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd95948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Adequate', 'Effective', 'Ineffective'], dtype='object')\n",
      "(33297, 49)\n",
      "Index(['Unnamed: 0', 'comma_count', 'NNP', ',', 'NN', 'VBP', 'VBG', 'TO', 'VB',\n",
      "       'IN', 'WRB', 'DT', 'VBZ', 'JJ', 'CC', 'EX', 'NNS', 'VBD', 'PRP', '.',\n",
      "       'VBN', 'RB', 'num_tokens', 'misspelled_count', 'PRP$', 'MD', 'CD', '``',\n",
      "       '''', 'WDT', 'WP', 'POS', 'NNPS', 'JJR', '(', ')', 'RP', ':', 'JJS',\n",
      "       'RBR', 'WP$', 'PDT', 'UH', '$', 'RBS', 'FW', '#', 'LS', 'SYM'],\n",
      "      dtype='object')\n",
      "Index(['Adequate', 'Effective', 'Ineffective'], dtype='object')\n",
      "(3468, 49)\n",
      "Index(['Unnamed: 0', 'comma_count', 'NNP', ',', 'NN', 'VBP', 'VBG', 'TO', 'VB',\n",
      "       'IN', 'WRB', 'DT', 'VBZ', 'JJ', 'CC', 'EX', 'NNS', 'VBD', 'PRP', '.',\n",
      "       'VBN', 'RB', 'num_tokens', 'misspelled_count', 'PRP$', 'MD', 'CD', '``',\n",
      "       '''', 'WDT', 'WP', 'POS', 'NNPS', 'JJR', '(', ')', 'RP', ':', 'JJS',\n",
      "       'RBR', 'WP$', 'PDT', 'UH', '$', 'RBS', 'FW', '#', 'LS', 'SYM'],\n",
      "      dtype='object')\n",
      "tensor([ 0.,  3.,  7.,  3., 12.,  2.,  2.,  1.,  2., 12.,  2.,  8.,  5.,  2.,\n",
      "         3.,  1.,  1.,  4.,  2.,  3.,  2.,  2., 76.,  3.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.]) tensor([0, 1, 0], dtype=torch.uint8)\n",
      "HEREH\n",
      "tensor([[2.7693e+04, 0.0000e+00, 2.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 1.5000e+01, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [8.0220e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.7680e+03, 3.0000e+00, 1.0000e+00, 3.0000e+00, 1.0000e+01, 1.0000e+00,\n",
      "         1.0000e+00, 2.0000e+00, 2.0000e+00, 7.0000e+00, 0.0000e+00, 7.0000e+00,\n",
      "         1.0000e+00, 2.0000e+00, 1.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.0000e+00, 3.0000e+00, 3.0000e+00, 4.8000e+01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.7049e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 3.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]) tensor([[0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path=''):\n",
    "        df_in = pd.read_csv(path)\n",
    "        df_orig_train_metadata = pd.read_csv('../data/processed/train_w_extracted_features_v2.csv')\n",
    "        df_in.set_index('discourse_id')\n",
    "        df_orig_train_metadata.set_index('discourse_id')\n",
    "        df_merged = pd.concat([df_in, df_orig_train_metadata], join='inner', axis=1)\n",
    "        \n",
    "        df_merged = df_merged[df_merged.columns.drop(list(df_merged.filter(regex='intersect')))]\n",
    "        df_merged = df_merged.fillna(0)\n",
    "        df_effectiveness = pd.get_dummies(df_in['discourse_effectiveness'])\n",
    "        #discourse_type_one_hot = pd.get_dummies(df_in['discourse_type'])\n",
    "        df_merged_with_discourse_type = df_merged\n",
    "        print(df_effectiveness.columns)\n",
    "        #df_merged_with_discourse_type[discourse_type_one_hot.columns] = discourse_type_one_hot\n",
    "        \n",
    "        df_merged_numeric = df_merged_with_discourse_type.select_dtypes(include=np.number)\n",
    "        print(df_merged_numeric.shape)\n",
    "        print(df_merged_numeric.columns)\n",
    "        \n",
    "        \n",
    "        self.x_data = torch.from_numpy(df_merged_numeric.to_numpy().astype(dtype=np.float32)) # size [n_samples, n_features]\n",
    "        self.y_data = torch.from_numpy(df_effectiveness[['Effective', 'Adequate', 'Ineffective']].to_numpy())\n",
    "        self.n_samples = self.x_data.shape[0]\n",
    "        \n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "train_dataset = FeaturesDataset('../data/train_berkeley.csv')\n",
    "test_dataset = FeaturesDataset('../data/test_berkeley.csv')\n",
    "\n",
    "# get first sample and unpack\n",
    "first_data = train_dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n",
    "\n",
    "# Load whole dataset with DataLoader\n",
    "# shuffle: shuffle data, good for training\n",
    "# num_workers: faster loading with multiple subprocesses\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "\n",
    "# convert to an iterator and look at one random sample\n",
    "train_iter = iter(train_loader)\n",
    "data = train_iter.next()\n",
    "features, labels = data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print('HEREH')\n",
    "print(features, labels)\n",
    "\n",
    "test_iter = iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c0bdd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc531c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size         \n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7cfeba",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a23f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # it automatically applies the softmax to the output of last layer. \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5d63a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db199804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SAMPLES =  33297 NUMBER OF ITERATIONS =  8325\n",
      "Epoch [1/10], Step [5000/8325], Loss: 2.5602\n",
      "Model Accuracy : 35.870818915801614 %\n",
      "Epoch [2/10], Step [5000/8325], Loss: 0.3853\n",
      "Model Accuracy : 26.038062283737023 %\n",
      "Epoch [3/10], Step [5000/8325], Loss: 2.0640\n",
      "Model Accuracy : 57.67012687427912 %\n",
      "Epoch [4/10], Step [5000/8325], Loss: 1.1917\n",
      "Model Accuracy : 57.641291810841984 %\n",
      "Epoch [5/10], Step [5000/8325], Loss: 0.9906\n",
      "Model Accuracy : 51.528258362168394 %\n",
      "Epoch [6/10], Step [5000/8325], Loss: 2.2077\n",
      "Model Accuracy : 57.266435986159166 %\n",
      "Epoch [7/10], Step [5000/8325], Loss: 1.7244\n",
      "Model Accuracy : 57.67012687427912 %\n",
      "Epoch [8/10], Step [5000/8325], Loss: 1.0037\n",
      "Model Accuracy : 57.67012687427912 %\n",
      "Epoch [9/10], Step [5000/8325], Loss: 1.4657\n",
      "Model Accuracy : 57.67012687427912 %\n",
      "Epoch [10/10], Step [5000/8325], Loss: 1.2843\n",
      "Model Accuracy : 57.75663206459054 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(\"NUMBER OF SAMPLES = \",str(total_samples) + \" NUMBER OF ITERATIONS = \",n_iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # here: 33297 samples, batch_size = 500, n_iters=33297/500=66.5 -> 67 iterations\n",
    "        # Run the training process        \n",
    "        \n",
    "        #Fixing \"RuntimeError: 1D target tensor expected, multi-target not supported\"\n",
    "        #print(outputs)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        #for t in inputs:\n",
    "        #    print(t.dtype)\n",
    "        #break\n",
    "        targets = np.argmax(labels, axis=1)\n",
    "        #print(targets)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        if (i+1) % 5000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_iterations}], Loss: {loss.item():.4f}')\n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            # max returns (value, index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Model Accuracy : {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f991d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b036b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy : 57.75663206459054 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        # max returns (value, index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Model Accuracy : {acc} %')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "80c80847a93213b192f70e81653c0285be2eeca283682b4031421ab3d792b00b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
